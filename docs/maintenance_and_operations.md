# Maintenance and Operational Procedures

## 1. Introduction

This document outlines the procedures for the ongoing maintenance and operation of the DICOM Backup System. Its purpose is to provide clear guidelines for ensuring system reliability, managing configurations, handling logs, performing backups of application components, and recovering from potential failures.

## 2. Logging Strategy

A robust logging strategy is essential for monitoring the application, diagnosing issues, and understanding its operational history.

### Log Levels

*   **Production Default:** `INFO`. This level provides a good balance of information about ongoing operations without excessive verbosity.
*   **Debugging:** The application should allow changing the log level to `DEBUG` (e.g., via a `LOG_LEVEL` environment variable). This is crucial for troubleshooting specific issues and should be used temporarily as needed.

### Log Format

*   **Recommendation:** Structured logging (e.g., JSON or key-value pairs) is highly recommended. This format allows for easier parsing, filtering, and analysis by automated tools and log management systems.
*   **Example Log Line Format:**
    ```
    TIMESTAMP LEVEL MODULE_NAME FUNCTION_NAME MESSAGE [key1=value1 key2="value with spaces" ...]
    ```
    *   `TIMESTAMP`: ISO 8601 format (e.g., `2023-10-27T14:35:07.123Z`).
    *   `LEVEL`: Log level (e.g., INFO, DEBUG, ERROR, WARNING).
    *   `MODULE_NAME`: The Python module where the log originated (e.g., `src.cli.backup`).
    *   `FUNCTION_NAME`: The function name, if applicable.
    *   `MESSAGE`: The main log message.
    *   `[key=value ...]` : Optional structured data relevant to the log entry (e.g., `SOPInstanceUID=1.2.3... source_aet=ARIA_PROD status=success`).

### Log Rotation

*   **Recommendation:** Implement daily rotation of application logs generated by the backup script.
*   **Tool:** Use standard OS tools like `logrotate` on Linux. Configure `logrotate` to:
    *   Rotate logs daily.
    *   Compress rotated logs (e.g., using gzip).
    *   Keep a specified number of rotated log files.
    *   Handle log file permissions.

### Log Retention

*   **Policy:** Define a clear log retention policy based on operational needs, storage capacity, and any compliance requirements.
    *   Example: Retain application logs on the Backup Application Host for **30-90 days**.
    *   Older logs can be archived to cheaper, long-term storage (e.g., cloud storage buckets) or discarded if no longer needed.
    *   If a centralized logging system is used, its retention policy will also apply.

### Centralized Logging (Conceptual)

*   **Recommendation:** For production environments, forward logs from the Backup Application Host (and ideally from the Orthanc servers - `orthanc-main` and `orthanc-staging`) to a centralized logging system.
    *   Examples: ELK Stack (Elasticsearch, Logstash, Kibana), Grafana Loki, Splunk.
*   **Benefits:**
    *   **Easier Searching & Filtering:** Query logs from all components in one place.
    *   **Analysis & Correlation:** Correlate events across different parts of the system (e.g., an error in the backup script with a specific log entry in Orthanc).
    *   **Dashboarding & Alerting:** Create dashboards based on log data and set up alerts for specific log patterns or error rates (as detailed in `monitoring_and_alerting.md`).

### Key Information to Log

The application should log the following key information to provide a comprehensive audit trail and facilitate troubleshooting:

*   Application start and stop events (including script invocation time).
*   Configuration loaded at startup (e.g., environment name, source system type). **Exclude sensitive data like passwords.**
*   Backup job initiation:
    *   Timestamp.
    *   Source system being processed (ARIA, MIM, Mosaiq).
    *   Key parameters used for the job (e.g., query criteria for ARIA/MIM, date for Mosaiq).
*   Backup job completion or failure:
    *   Status (success/failure).
    *   Total duration.
    *   Number of instances processed.
    *   Number of instances successfully backed up.
    *   Number of instances failed (if applicable).
*   For each DICOM instance processed:
    *   SOPInstanceUID.
    *   Original source AE.
    *   Intended destination AE (e.g., staging SCP, main Orthanc).
    *   Status of the transfer operation (e.g., C-STORE to staging success/failure, C-MOVE to main Orthanc success/failure).
    *   Status of verification (e.g., found in Orthanc, data match success/failure).
*   Errors and exceptions:
    *   Full stack traces for unhandled exceptions.
    *   Specific error messages for known error conditions (e.g., DICOM association failure, database query failure).
*   DICOM association activity:
    *   Attempts to establish associations (with peer AET, host, port).
    *   Success or failure of association establishment.
    *   Association release events.
*   Key decision points or branches in the workflow (e.g., "No instances found for source X, skipping transfer.").
*   Retry attempts for operations (e.g., "Retrying C-FIND for UID Y, attempt 2/3").

## 3. Backup and Disaster Recovery (Application & Configuration)

### Scope

This section focuses on the backup and recovery of the DICOM Backup System application itself: its code, configuration files, and related operational scripts. It **does not** cover the backup and disaster recovery of the DICOM data stored within the Orthanc servers; that is the responsibility of the Orthanc system's own backup procedures.

### Application Code

*   **Source Control (Primary Backup):**
    *   The application code is maintained in a Git version control system.
    *   Regular pushes of changes (features, bug fixes) to a central Git repository (e.g., GitHub, GitLab, Bitbucket) serve as the primary backup of the codebase.
*   **Deployment Artifacts (If Applicable):**
    *   If the deployment process involves creating specific build artifacts (e.g., Python wheels, packaged tarballs, Docker images), these artifacts should be:
        *   Versioned (e.g., tagged consistently with code releases).
        *   Stored in a secure artifact repository (e.g., Artifactory, Nexus, GitHub Packages, Docker Hub/ECR).

### Configuration Files

*   **`environments.toml`**: This is the primary configuration file containing all environment-specific settings (DICOM AEs, DB connections, operational parameters) and should be stored in Git.
*   **`dicom.toml`**: This file is deprecated and should not be relied upon for critical configuration. Its settings have been migrated to `environments.toml`.
*   **Production Environment Variables/Secrets:**
    *   **Non-Sensitive Variables:** OS-level environment variables are primarily for settings like `LOG_LEVEL` or deployment-specific switches (e.g., enabling/disabling features if applicable). Most non-sensitive configurations (like AE titles, IPs, ports, SQL queries, `max_uids_per_run`) are now defined directly within `environments.toml`.
    *   **Sensitive Values (Secrets):**
        *   Passwords (e.g., `MOSAIQ_DB_PASSWORD`), API keys, or any other sensitive credentials **MUST NOT** be stored directly in Git or non-secure configuration files.
        *   These must be managed using a dedicated secrets management system (e.g., HashiCorp Vault, AWS Secrets Manager, Azure Key Vault, Google Cloud Secret Manager).
        *   The backup of the secrets management system itself is a critical infrastructure task, usually handled by its provider or specific operational procedures defined for that system.
    *   **Secret Mapping Records:** Maintain a secure (and potentially audited) record of which secrets (by name or reference, not value) are applied to which production environment or application instance. This is crucial for redeployment and DR.

### Disaster Recovery (Application Host)

This outlines steps to recover the Backup Application Host if it becomes completely unavailable (e.g., VM corruption, hardware failure).

*   **Scenario:** The physical or virtual host running the DICOM Backup System application is lost or irrecoverable.
*   **Recovery Steps:**
    1.  **Provision New Host:**
        *   Set up a new Virtual Machine (VM) or container instance with the same operating system specifications (e.g., Ubuntu Server LTS version) and resource allocation (CPU, memory, disk) as the failed host.
        *   Ensure basic network connectivity and security configurations (firewalls, access controls) are applied.
    2.  **Install Dependencies:**
        *   Install Python (same version as previously used, e.g., 3.11).
        *   Install any OS-level packages required by the application or its Python dependencies (e.g., ODBC drivers for Mosaiq).
    3.  **Deploy Application Code:**
        *   Clone the latest stable release tag (or specific commit for rollback) of the application code from the central Git repository.
        *   Alternatively, if deployment artifacts are used, download and deploy the appropriate versioned artifact from the artifact repository.
    4.  **Restore Application Configuration:**
        *   **`toml` Files:** Deploy `environments.toml` from version control. Clarify that `dicom.toml` is deprecated and its relevant settings should have been migrated to `environments.toml`.
        *   **Environment Variables:**
            *   Apply non-sensitive environment variables (e.g., `LOG_LEVEL`, `PYTHONPATH`) through startup scripts or the new host's environment configuration mechanism. AE titles, hostnames, and ports are now primarily managed in `environments.toml`.
            *   **Secrets:** Configure the application or its environment to securely fetch the necessary secrets (e.g., `MOSAIQ_DB_PASSWORD`) from the secrets management system. This might involve setting up access roles/permissions for the new host/application instance to the secrets manager.
    5.  **Set up Scheduling:**
        *   Re-configure the `cron` job (or other scheduler) to execute the backup script (`src/cli/backup.py`) with the required frequency and arguments.
    6.  **Perform Health Checks & Testing:**
        *   Verify basic application functionality.
        *   Test DICOM connectivity to all source systems (ARIA, MIM), the staging SCP, and the main Orthanc backup server using `dicom_utils.py echo` or similar.
        *   Test database connectivity to Mosaiq.
        *   Run a small, controlled test backup job for each source type to ensure end-to-end functionality.
        *   Monitor logs for any errors.

## 4. Scheduled Maintenance

Regularly scheduled maintenance is vital for system stability, security, and performance.

### Backup Application Host

*   **OS Patching:**
    *   Apply operating system security patches and updates regularly (e.g., monthly, or in line with the organization's security policy). This should be done in a maintenance window, and a snapshot/backup of the host should be considered before major patching.
*   **Dependency Review:**
    *   Periodically (e.g., quarterly or semi-annually) review Python dependencies listed in `requirements.txt` for known security vulnerabilities or critical updates. Use tools like `pip-audit` or GitHub's Dependabot. Update dependencies in a controlled manner, testing thoroughly in a staging environment first.
*   **Log File Management:**
    *   Ensure log rotation (e.g., via `logrotate`) is configured and functioning correctly.
    *   Monitor disk space used by application logs.
    *   Manually archive or clear older logs if necessary and if not automatically managed by rotation/retention policies, especially if a centralized logging system is not yet fully implemented or if local copies are kept for a short period.
*   **Resource Monitoring Review:**
    *   Regularly (e.g., monthly) review trends in CPU, memory, disk I/O, and network usage on the Backup Application Host (using data from the monitoring system).
    *   This helps anticipate future capacity needs and identify potential performance degradation before it becomes critical.

### Orthanc Servers (Main and Staging - Operational Considerations for the App)

While the full maintenance of Orthanc servers is a separate infrastructure task, the Backup Application's operation depends on them. Therefore, these aspects are critical from the application's perspective:

*   **Disk Space Monitoring:**
    *   This is **crucial**. The backup application relies on the main Orthanc server (`orthanc-main`) and the Mosaiq staging SCP (`orthanc-staging`) having sufficient free disk space to receive new DICOM instances.
    *   This should be a primary alert condition configured in the monitoring system (as detailed in `monitoring_and_alerting.md`).
*   **Orthanc Logs:**
    *   Periodically (e.g., weekly, or when troubleshooting) review the Orthanc server logs on both `orthanc-main` and `orthanc-staging` for errors, warnings, or performance issues that might be relevant to backup operations (e.g., C-STORE failures, association problems, database issues).
*   **Orthanc Backups (Verification of Process):**
    *   Periodically confirm with the infrastructure/Orthanc administration team that the Orthanc instances themselves (their internal SQLite database or PostgreSQL backend, and their DICOM file storage) are being backed up according to their own dedicated, robust backup procedures. The DICOM Backup System *sends* data to Orthanc; it does not manage Orthanc's internal backups.

### Configuration Review

*   **Periodically (e.g., quarterly or before major system changes in connected systems):**
    *   Review all application and environment configurations. This includes:
        *   AE Titles, IP addresses, and port numbers for all source systems (ARIA, MIM, Mosaiq DB), the staging SCP, and the main Orthanc backup server, as defined in `src/config/environments.toml` for each configured environment.
        *   Default sources, default backup targets, and other settings within each environment block in `environments.toml`.
        *   SQL queries used for Mosaiq (e.g., `mosaiq_backup_sql_query` in the `[environment.settings]` section of `environments.toml`).
        *   Log level and rotation settings.
    *   Ensure these configurations are still accurate and align with the current state of the production environment and connected clinical systems.
    *   Update documentation if any standard operational configurations change.
